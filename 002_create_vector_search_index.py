# Databricks notebook source
# MAGIC %pip install -U -qqqq langchain langgraph databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

import pkg_resources

# List of packages to check
packages_to_check = [
    "langchain",
    "langgraph",
    "databricks-langchain",
    "pydantic",
    "databricks-agents",
    "unitycatalog-langchain",
    "uv"
]

# Get installed packages and their versions
installed_packages = {d.project_name: d.version for d in pkg_resources.working_set}

# Check and print the version of specified packages
for package_name in packages_to_check:
    version = installed_packages.get(package_name)
    if version:
        print(f"{package_name}: {version}")
    else:
        print(f"{package_name} is not installed.")

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c
import mlflow
from mlflow.deployments import get_deploy_client
from databricks.vector_search.client import VectorSearchClient
from pprint import pprint
import json
from pyspark.sql.functions import to_json, col, pandas_udf
import pandas as pd
import yaml

# COMMAND ----------

deploy_client = get_deploy_client("databricks")
vsc = VectorSearchClient()

# COMMAND ----------

# gte-large-en Foundation models are available using the /serving-endpoints/databricks-gtegte-large-en/invocations api. 
deploy_client = get_deploy_client("databricks")

## NOTE: if you change your embedding model here, make sure you change it in the query step too
embeddings = deploy_client.predict(endpoint="databricks-gte-large-en", inputs={"input": ["What is organization"]})
pprint(embeddings)

# COMMAND ----------

@pandas_udf("array<float>")
def get_embedding(contents: pd.Series) -> pd.Series:
    import mlflow.deployments
    deploy_client = mlflow.deployments.get_deploy_client("databricks")
    def get_embeddings(batch):
        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) 
        response = deploy_client.predict(endpoint="databricks-gte-large-en", inputs={"input": batch})
        return [e['embedding'] for e in response.data]

    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.
    max_batch_size = 150
    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]

    # Process each batch and collect the results
    all_embeddings = []
    for batch in batches:
        all_embeddings += get_embeddings(batch.tolist())

    return pd.Series(all_embeddings)

# COMMAND ----------

# MAGIC %run ./000_initialize

# COMMAND ----------

# spark.sql(f"""
#           drop table {embedded_table_fullname}
#           """)

# COMMAND ----------

spark.sql(f"""
          CREATE TABLE IF NOT EXISTS {embedded_table_fullname} (
            id BIGINT GENERATED BY DEFAULT AS IDENTITY,
            conversation_id STRING,
            content STRING,
            name STRING,
            embedding ARRAY <FLOAT>
            ) TBLPROPERTIES (delta.enableChangeDataFeed = true)
          """)

# COMMAND ----------

(spark.readStream.table(f'{catalog_name}.{schema_name}.{source_table_for_embedding}')
      .withColumn('content', to_json(col('participant')))
      .withColumn("embedding", get_embedding("content"))
      .selectExpr('conversation_id', 'content', 'name', 'embedding')
  .writeStream
    .trigger(availableNow=True)
    .option("mergeSchema", "true")
    .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/pdf_chunk')
    .table(f'{embedded_table_fullname}').awaitTermination())

# COMMAND ----------

spark.table(embedded_table_fullname).display()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### Index Creation

# COMMAND ----------

vsc.create_delta_sync_index(
      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
      index_name=vs_index_fullname,
      source_table_name=embedded_table_fullname,
      pipeline_type="TRIGGERED", #Sync needs to be manually triggered
      primary_key="id",
      embedding_dimension=1024, #Match your model embedding size (gte)
      embedding_vector_column="embedding"
    )

# COMMAND ----------

vs_index_fullname

# COMMAND ----------

question = "Give me the how many meetings were conducted by Emma"

response = deploy_client.predict(endpoint="databricks-gte-large-en", inputs={"input": [question]})
embeddings = [e['embedding'] for e in response.data]

results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(
  query_vector=embeddings[0],
  query_text=question,
  columns=["conversation_id", "content", "name"],
  num_results=10,
  query_type='hybrid'
  )
docs = results.get('result', {}).get('data_array', [])
pprint(docs)